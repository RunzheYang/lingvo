{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVcYJIEwd3As"
      },
      "source": [
        "#### local run command\n",
        "`blaze run -c opt learning/brain/research/babelfish/colab:colab_notebook --define=babelfish_task=multimodal`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd7ZcyyqZKJi"
      },
      "outputs": [],
      "source": [
        "import lingvo.compat as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pprint\n",
        "import os\n",
        "\n",
        "from lingvo.core import py_utils\n",
        "from google3.learning.brain.research.babelfish import tokenizers\n",
        "from google3.learning.brain.research.babelfish.multimodal.params.experimental import nlu_baselines as nlu_params\n",
        "\n",
        "# from google3.pyglib import gfiler\n",
        "\n",
        "from google3.perftools.accelerators.xprof.api.colab import xprof\n",
        "\n",
        "tf.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7ZujlTGZZlK"
      },
      "outputs": [],
      "source": [
        "mdl_mixed = nlu_params.SST2ClassificationMixed()\n",
        "\n",
        "mdl_mixed.DROPOUT_RATE = 0.0\n",
        "\n",
        "p_mixed = mdl_mixed.Task()\n",
        "\n",
        "# Note: We use the name as part of var/name scopes, you need to ensure that\n",
        "# the name here matches for checkpoints to load successfully.\n",
        "\n",
        "p_mixed.encoder_ex.shared_emb_ex.softmax.use_num_classes_major_weight = True\n",
        "p_mixed.decoder_ex.shared_emb_ex.softmax.use_num_classes_major_weight = True\n",
        "\n",
        "p_mixed.name = 'MixedFinetune'\n",
        "\n",
        "p_mixed.input = mdl_mixed.Train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CafizIxXQfa"
      },
      "outputs": [],
      "source": [
        "# We are going to use the global graph for this entire colab.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Instantiate the Task.\n",
        "task_mixed = p_mixed.Instantiate()\n",
        "\n",
        "# Create variables by running FProp.\n",
        "_ = task_mixed.FPropDefaultTheta()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcdhCnAGprFr"
      },
      "outputs": [],
      "source": [
        "# Create a new session and initialize all the variables.\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCpH22KqYgVQ"
      },
      "outputs": [],
      "source": [
        "# Setup the checkpoint loading rules for OverrideVarsFromCheckpoints.\n",
        "loading_rules_mixed = [\n",
        "    (\n",
        "        \"MixedFinetune/(.*/var:0$)\",  \n",
        "        \"MixedFinetune/%s\"    \n",
        "    )\n",
        "]\n",
        "\n",
        "ignore_rules = []  # No ignore rules, parse all saved vars.\n",
        "\n",
        "ckpts_loading_rules = lambda x, y:{\n",
        "    x: (y, ignore_rules)\n",
        "}\n",
        "\n",
        "ignore_rules = []  # No ignore rules, parse all saved vars.\n",
        "ckpt_path_mixed = '/cns/tp-d/home/runzheyang/brain/rs=6.3/sst2.mixed.small.encdec_it2t/train/ckpt-00010000'\n",
        "\n",
        "# Load the saved checkpoint into the session.\n",
        "py_utils.OverrideVarsFromCheckpoints(\n",
        "    tf.all_variables(p_mixed.name+\"//*\"), ckpts_loading_rules(ckpt_path_mixed, loading_rules_mixed))(sess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmKbE6-5DkN4"
      },
      "source": [
        "### Load top 5000 frequent words and Task examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpVjYew3u1RU"
      },
      "outputs": [],
      "source": [
        "from google3.pyglib import gfile\n",
        "with gfile.Open('/cns/tp-d/home/runzheyang/brain/rs=6.3/data/5000-words.txt', 'r') as f:\n",
        "  freq_words = f.read()\n",
        "\n",
        "import json\n",
        "with gfile.Open('/cns/tp-d/home/runzheyang/brain/rs=6.3/data/sst2_validaiton', 'r') as fh:  \n",
        "  all_ex = json.load(fh)\n",
        "with gfile.Open('/cns/tp-d/home/runzheyang/brain/rs=6.3/data/sst2_it2t_failue', 'r') as fh:  \n",
        "  it2t_ex = json.load(fh)\n",
        "with gfile.Open('/cns/tp-d/home/runzheyang/brain/rs=6.3/data/sst2_t2t_failue', 'r') as fh:  \n",
        "  t2t_ex = json.load(fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn9_kRol2Pct"
      },
      "outputs": [],
      "source": [
        "input_p = mdl_mixed.Train()\n",
        "input_gen = input_p.Instantiate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "divXII6i8kA5"
      },
      "outputs": [],
      "source": [
        "freq_ids = input_gen._vocabulary._encode(freq_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaSZWPEeG5cl"
      },
      "outputs": [],
      "source": [
        "freq_ids = np.unique(freq_ids)\n",
        "freq_ids = [int(i) for i in freq_ids]\n",
        "len(freq_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z_P52911U1m"
      },
      "outputs": [],
      "source": [
        "all_ex_tokens = []\n",
        "for ex in all_ex:\n",
        "  all_ex_tokens += input_gen._vocabulary._encode(ex)\n",
        "\n",
        "all_ex_tokens = [int(ids) for ids in np.unique(all_ex_tokens)]\n",
        "len(all_ex_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF5Q7a_y1dVm"
      },
      "outputs": [],
      "source": [
        "# check the overlap between top 5000 words and SST tokens\n",
        "len(np.intersect1d(all_ex_tokens, freq_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCOoMDfD1iWo"
      },
      "outputs": [],
      "source": [
        "freq_ids = [int(ids) for ids in np.union1d(freq_ids, all_ex_tokens)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4zdWIqaCgup"
      },
      "outputs": [],
      "source": [
        "input_gen._vocabulary._decode(freq_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJzqXdtEC0NE"
      },
      "outputs": [],
      "source": [
        "len(freq_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKq6N3QcDDcL"
      },
      "outputs": [],
      "source": [
        "# get token embedding (w/o positional embedding), assuming share_emd=True.\n",
        "it2t_token_embeddings = task_mixed.encoder.softmax.EmbLookup(\n",
        "    task_mixed.theta.encoder.softmax, freq_ids)\n",
        "\n",
        "t2t_token_embeddings = task_mixed.encoder_ex.softmax.EmbLookup(\n",
        "    task_mixed.theta.encoder_ex.softmax, freq_ids)\n",
        "\n",
        "selector_input = tf.concat([it2t_token_embeddings, t2t_token_embeddings], axis=1)\n",
        "# Select between IT2T and T2T embeddings\n",
        "selection = task_mixed.emb_selector.FProp(task_mixed.theta.emb_selector, selector_input)\n",
        "\n",
        "fetches = py_utils.NestedMap({\n",
        "    \"it2t_emb\": it2t_token_embeddings,\n",
        "    \"t2t_emb\": t2t_token_embeddings,\n",
        "    \"selection\": selection})\n",
        "\n",
        "print(fetches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twMbz_cyDZhD"
      },
      "outputs": [],
      "source": [
        "emb_output = sess.run(fetches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpLY7b2Z-km0"
      },
      "outputs": [],
      "source": [
        "it2t_emb = emb_output[\"it2t_emb\"]\n",
        "t2t_emb = emb_output[\"t2t_emb\"]\n",
        "selection = emb_output[\"selection\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps7LLyCigNyk"
      },
      "outputs": [],
      "source": [
        "selection.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JipMQSwgSBW"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set_context(\"talk\")\n",
        "\n",
        "sns.distplot(selection, kde=False,\n",
        "            kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\", \"color\":\"g\"},\n",
        "            hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\n",
        "                            \"alpha\": 1, \"color\": \"g\"})\n",
        "plt.xlabel(\"selection (t2t)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKsGOoa80XFT"
      },
      "outputs": [],
      "source": [
        "def cos_similarity(vecs):\n",
        "  dotp = vecs.dot(vecs.T)\n",
        "  norm = np.sqrt((vecs ** 2).sum(1))\n",
        "  length = np.outer(norm, norm)\n",
        "  return dotp / length - np.eye(len(vecs))\n",
        "\n",
        "def k_nn(sim_matrix, k=-1):\n",
        "  return np.argsort(sim_matrix, -1)[:,::-1][:, :k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzWKP-TQC2cC"
      },
      "outputs": [],
      "source": [
        "# obtain top 10 similar words\n",
        "sim_matrix_it2t = cos_similarity(it2t_emb)\n",
        "knn_it2t_10 = k_nn(sim_matrix_it2t, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJvuoGQFEJff"
      },
      "outputs": [],
      "source": [
        "# obtain top 10 similar words\n",
        "sim_matrix_t2t = cos_similarity(t2t_emb)\n",
        "knn_t2t_10 = k_nn(sim_matrix_t2t, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWx_q3S1RVyP"
      },
      "outputs": [],
      "source": [
        "# obtain top 5 similar words\n",
        "knn_it2t_5 = k_nn(sim_matrix_it2t, 5)\n",
        "knn_t2t_5 = k_nn(sim_matrix_t2t, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8vdcN6BKqN_"
      },
      "source": [
        "### Check nearest words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVNOy37XEUX0"
      },
      "outputs": [],
      "source": [
        "def check_id(rid, freq_ids, knn):\n",
        "  print(\"query:\", input_gen._vocabulary._decode([int(freq_ids[rid])]))\n",
        "  print(\"similar words:\", [input_gen._vocabulary._decode([int(freq_ids[i])])  for i in knn[rid]])\n",
        "\n",
        "def check_word(word, freq_ids, knn):\n",
        "  wids = input_gen._vocabulary._encode(word)\n",
        "  for wid in wids:\n",
        "    # skip the empty token..\n",
        "    if wid == 3: continue\n",
        "    rid = freq_ids.index(wid)\n",
        "    check_id(rid, freq_ids, knn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADKbpGQdE8Lz"
      },
      "outputs": [],
      "source": [
        "query = 'cat'\n",
        "check_word(query, freq_ids, knn_it2t_10)\n",
        "check_word(query, freq_ids, knn_t2t_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Pl0YiduJgwB"
      },
      "outputs": [],
      "source": [
        "query = 'dream'\n",
        "check_word(query, freq_ids, knn_it2t_10)\n",
        "check_word(query, freq_ids, knn_t2t_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl8zsTPgJoJX"
      },
      "outputs": [],
      "source": [
        "query = 'throw'\n",
        "check_word(query, freq_ids, knn_it2t_10)\n",
        "check_word(query, freq_ids, knn_t2t_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8SEGl8OKMld"
      },
      "outputs": [],
      "source": [
        "query = 'sing'\n",
        "check_word(query, freq_ids, knn_it2t_10)\n",
        "check_word(query, freq_ids, knn_t2t_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKShf-KhKkEL"
      },
      "outputs": [],
      "source": [
        "query = 'compromise'\n",
        "check_word(query, freq_ids, knn_it2t_10)\n",
        "check_word(query, freq_ids, knn_t2t_10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkSmS65ANfiR"
      },
      "source": [
        "### Quantitive comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hDdZV4VLH6E"
      },
      "outputs": [],
      "source": [
        "def diff_scores(knn1, knn2, k):\n",
        "  return [len(np.intersect1d(knn1[i], knn2[i]))/k for i in range(len(knn1))] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii_YLZwjNrh5"
      },
      "outputs": [],
      "source": [
        "top_k = 10\n",
        "it2t_vs_t2t = diff_scores(k_nn(sim_matrix_it2t, top_k), k_nn(sim_matrix_t2t, top_k), top_k)\n",
        "np.mean(it2t_vs_t2t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXVdwLgC32L4"
      },
      "outputs": [],
      "source": [
        "top_ks = [1, 5, 10, 20, 50, 100, 200, 500]\n",
        "it2t_vs_t2t_k = {k:diff_scores(k_nn(sim_matrix_it2t, k), k_nn(sim_matrix_t2t, k), k) for k in top_ks}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pccifyuE4nPp"
      },
      "outputs": [],
      "source": [
        "[np.mean(it2t_vs_t2t_k[k]) for k in top_ks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ8dzF6z4TCx"
      },
      "outputs": [],
      "source": [
        "plt.plot(top_ks, [np.mean(it2t_vs_t2t_k[k]) for k in top_ks])\n",
        "plt.xlabel(\"Number of neighbors (k)\")\n",
        "plt.ylabel(\"Avg. coherence Score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wz5iXQJOPX8"
      },
      "outputs": [],
      "source": [
        "def id2word(rid):\n",
        "  return(input_gen._vocabulary._decode([int(freq_ids[rid])]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qda1r-N4OjN_"
      },
      "outputs": [],
      "source": [
        "# most dissimilar words (it2t vs t2t)\n",
        "np.vectorize(id2word)(np.argsort(it2t_vs_t2t))[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Air-2b3NPJXr"
      },
      "outputs": [],
      "source": [
        "# most similar words (it2t vs t2t)\n",
        "np.vectorize(id2word)(np.argsort(it2t_vs_t2t))[-30:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PedDQddPLHk"
      },
      "outputs": [],
      "source": [
        "query = 'abstract'\n",
        "check_word(query, freq_ids, knn_it2t_10)\n",
        "check_word(query, freq_ids, knn_t2t_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEvCO7b58X3S"
      },
      "outputs": [],
      "source": [
        "query = 'decrease'\n",
        "check_word(query, freq_ids, knn_it2t_10)\n",
        "check_word(query, freq_ids, knn_t2t_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvWmyf4mHd9D"
      },
      "outputs": [],
      "source": [
        "query = 'traditional'\n",
        "check_word(query, freq_ids, knn_it2t_10)\n",
        "check_word(query, freq_ids, knn_t2t_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU0l5zRL2Mo1"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set_context('talk')\n",
        "\n",
        "distplot = lambda x, c, l: sns.distplot(x, kde=True,\n",
        "                            kde_kws={\"lw\": 0,  \"alpha\": 0.0},\n",
        "                            hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\n",
        "                            \"alpha\": 0.8, \"color\": c, \"label\": l})\n",
        "\n",
        "distplot(it2t_vs_t2t_k[5], 'orange', 'k=5')\n",
        "distplot(it2t_vs_t2t_k[10], 'g', 'k=10')\n",
        "distplot(it2t_vs_t2t_k[100], 'b', 'k=100')\n",
        "distplot(it2t_vs_t2t_k[500], 'r', 'k=500')\n",
        "\n",
        "plt.xlabel(\"top 10 nearest neighbor coherence\")\n",
        "plt.legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJqeJNTK2X7P"
      },
      "source": [
        "# Compared with concreteness scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBGND9Rx2gUE"
      },
      "outputs": [],
      "source": [
        "from google3.pyglib import gfile\n",
        "import pandas as pd\n",
        "\n",
        "with gfile.Open('/cns/tp-d/home/runzheyang/brain/rs=6.3/data/concreteness.xlsx', 'rb') as fh:  \n",
        "  concrete_scores = pd.read_excel(fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrzvytHSEYW9"
      },
      "outputs": [],
      "source": [
        "concrete_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xoIeVREFQQ6"
      },
      "outputs": [],
      "source": [
        "bool_wordpiece = []\n",
        "for i, w in enumerate(list(concrete_scores[\"Word\"])):\n",
        "  ids = input_gen._vocabulary.encode(str(w))\n",
        "  bool_wordpiece.append(len(ids) == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7g1tYWtIL4z"
      },
      "outputs": [],
      "source": [
        "concrete_scores['is_wordpiece'] = bool_wordpiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqy-AqqGR2uQ"
      },
      "outputs": [],
      "source": [
        "concrete_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO_VPygeUgY7"
      },
      "outputs": [],
      "source": [
        "cr_wid = [input_gen._vocabulary.encode(w) for w in concrete_scores[\"Word\"][concrete_scores[\"is_wordpiece\"]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2drAGsjMUyEp"
      },
      "outputs": [],
      "source": [
        "cr_wid = np.array(cr_wid).reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYKmFYgvVTeF"
      },
      "outputs": [],
      "source": [
        "len(np.intersect1d(cr_wid, freq_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKryZfGLWCA4"
      },
      "outputs": [],
      "source": [
        "len(np.union1d(cr_wid, freq_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4DwJ-fDVePA"
      },
      "outputs": [],
      "source": [
        "len(np.unique(cr_wid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG7ul72l2gwt"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "def r2(x, y):\n",
        "    return stats.pearsonr(x, y)[0] ** 2\n",
        "\n",
        "sns.regplot(emb_output[\"selection\"], it2t_vs_t2t, color='green', scatter_kws={'alpha':0.1})\n",
        "plt.xlabel(\"selection (t2t)\")\n",
        "plt.ylabel(\"it2t vs t2t coherence score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1REPp5T8Xoc8"
      },
      "outputs": [],
      "source": [
        "conc_m = concrete_scores[\"Conc.M\"][concrete_scores[\"is_wordpiece\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnXPNPF8HtB_"
      },
      "outputs": [],
      "source": [
        "len(conc_m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-_RFJEjEuy8"
      },
      "outputs": [],
      "source": [
        "len(freq_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVTd4juUXu-H"
      },
      "outputs": [],
      "source": [
        "cr_it2t_vs_t2t = [it2t_vs_t2t[freq_ids.index(ids)] for ids in cr_wid if ids in freq_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEFn1ehG40v_"
      },
      "outputs": [],
      "source": [
        "cr_selection = [float(emb_output[\"selection\"][freq_ids.index(ids)]) for ids in cr_wid if ids in freq_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57TSgwaaC9_4"
      },
      "outputs": [],
      "source": [
        "conc_m = conc_m[[(w in freq_ids) for w in cr_wid]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkdRLDjB8szh"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.data.path.append('/usr/local/google/home/runzheyang/nltk_data')\n",
        "get_pos = lambda x: nltk.pos_tag(nltk.word_tokenize(input_gen._vocabulary.decode([int(x)])))[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6Tb-hIIFFy5"
      },
      "outputs": [],
      "source": [
        "POS = [get_pos(ids) for ids in cr_wid if ids in freq_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DfBbvMpFF1u"
      },
      "outputs": [],
      "source": [
        "np.unique(POS, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OldHaj1AFF56"
      },
      "outputs": [],
      "source": [
        "sns.histplot(POS)\n",
        "plt.xticks(rotation=70)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTRzZ1lfLZav"
      },
      "outputs": [],
      "source": [
        "cr_it2t_vs_t2t = np.array(cr_it2t_vs_t2t)\n",
        "conc_m = np.array(conc_m)\n",
        "POS = np.array(POS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP9Gosaec96y"
      },
      "outputs": [],
      "source": [
        "is_in = lambda x, y: [x_ in y for x_ in x] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKwycmPbYSjP"
      },
      "outputs": [],
      "source": [
        "sns.regplot(conc_m, cr_it2t_vs_t2t, scatter_kws={'alpha':0.1})\n",
        "pos_set = ['VB', 'VBD', 'VBG', 'VBN']\n",
        "sns.regplot(conc_m[is_in(POS, pos_set)], cr_it2t_vs_t2t[is_in(POS, pos_set)], color='red', scatter_kws={'alpha':0.1})\n",
        "plt.ylabel(\"it2t vs t2t coherence score\")\n",
        "plt.xlabel(\"concreteness\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXnLlKJu8xge"
      },
      "outputs": [],
      "source": [
        "sns.regplot(conc_m, cr_selection, scatter_kws={'alpha':0.05})\n",
        "plt.ylabel(\"selection (t2t)\")\n",
        "plt.xlabel(\"concreteness\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY42kkUX5Q0Z"
      },
      "outputs": [],
      "source": [
        "np.sqrt(r2(conc_m, cr_selection))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMLb-3Fs8j05"
      },
      "outputs": [],
      "source": [
        "# sns.regplot(conc_m, cr_selection, scatter_kws={'alpha':0.1})\n",
        "pos_set = ['VB', 'VBD', 'VBG', 'VBN']\n",
        "sns.regplot(conc_m[is_in(POS, pos_set)], np.array(cr_selection)[is_in(POS, pos_set)], color='red', scatter_kws={'alpha':0.1})\n",
        "plt.ylabel(\"selection (t2t)\")\n",
        "plt.xlabel(\"concreteness\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTaxQ3CUgraI"
      },
      "outputs": [],
      "source": [
        "np.sqrt(r2(conc_m[is_in(POS, pos_set)], np.array(cr_selection)[is_in(POS, pos_set)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFmJ0i45bYuN"
      },
      "outputs": [],
      "source": [
        "# sns.regplot(conc_m, cr_selection, scatter_kws={'alpha':0.1})\n",
        "pos_set = ['NN', 'NNS']\n",
        "sns.regplot(conc_m[is_in(POS, pos_set)], np.array(cr_selection)[is_in(POS, pos_set)], color='orange', scatter_kws={'alpha':0.1})\n",
        "plt.ylabel(\"selection (t2t)\")\n",
        "plt.xlabel(\"concreteness\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVUGAY5g76JX"
      },
      "source": [
        "## Compare SST2 Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjVan9hBA8re"
      },
      "outputs": [],
      "source": [
        "def seq_score(sentence_ids, freq_ids, score):\n",
        "  rids = []\n",
        "  for wid in sentence_ids:\n",
        "    if wid in freq_ids:\n",
        "      rids.append(freq_ids.index(wid))\n",
        "      scores = np.array(score)[rids]\n",
        "  return scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnDWobD9SGH2"
      },
      "outputs": [],
      "source": [
        "def seq_inconsist_words(sentence_ids, freq_ids, score):\n",
        "  rids = []\n",
        "  for wid in sentence_ids:\n",
        "    if wid in freq_ids:\n",
        "      rids.append(freq_ids.index(wid))\n",
        "      scores = np.array(score)[rids]\n",
        "  return 100*(scores \u003c= 0.2).sum()/(scores \u003c= 1.1).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2V_m5KsD2Le"
      },
      "outputs": [],
      "source": [
        "def exs_scores(exs, k):\n",
        "  _ids = [input_gen._vocabulary._encode(ex) for ex in exs]\n",
        "  _scores = [seq_score(_ids[i], freq_ids, it2t_vs_t2t_k[k]) for i in range(len(exs))]\n",
        "  return _scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6Emv-PLSbgm"
      },
      "outputs": [],
      "source": [
        "def exs_icwords(exs, k):\n",
        "  _ids = [input_gen._vocabulary._encode(ex) for ex in exs]\n",
        "  _scores = [seq_inconsist_words(_ids[i], freq_ids, it2t_vs_t2t_k[k]) for i in range(len(exs))]\n",
        "  return _scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK1X9J--BDKa"
      },
      "outputs": [],
      "source": [
        "top_k = 20\n",
        "\n",
        "val_scores = exs_scores(all_ex, top_k)\n",
        "it2t_ex_scores = exs_scores(it2t_ex, top_k)\n",
        "t2t_ex_scores = exs_scores(t2t_ex, top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqvXgMEkDpBz"
      },
      "outputs": [],
      "source": [
        "correct_ex = [ex for ex in all_ex if ex not in it2t_ex and ex not in t2t_ex]\n",
        "correct_scores = exs_scores(correct_ex, top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P49p1s2w_Fh-"
      },
      "outputs": [],
      "source": [
        "common_ex = [ex for ex in all_ex if ex in it2t_ex and ex in t2t_ex]\n",
        "common_scores = exs_scores(common_ex, top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJKDMHewEYo5"
      },
      "outputs": [],
      "source": [
        "it2t_only_ex = [ex for ex in it2t_ex if ex not in t2t_ex]\n",
        "it2t_only_scores = exs_scores(it2t_only_ex, top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8svh9oxZFqop"
      },
      "outputs": [],
      "source": [
        "t2t_only_ex = [ex for ex in t2t_ex if ex not in it2t_ex]\n",
        "t2t_only_scores = exs_scores(t2t_only_ex, top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifk6i3IiG0OZ"
      },
      "outputs": [],
      "source": [
        "it2t_ex, t2t_ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTRlm2tFDslZ"
      },
      "outputs": [],
      "source": [
        "len(np.intersect1d(it2t_ex, t2t_ex)), len(it2t_only_ex), len(t2t_only_ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SnyZd87DJvW"
      },
      "outputs": [],
      "source": [
        "distplot = lambda x, b, c, l: sns.distplot(x, kde=True, bins=b,\n",
        "                            kde_kws={\"color\": c, \"lw\": 3, \"alpha\": 0.2},\n",
        "                            hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\n",
        "                            \"alpha\": 0.2, \"color\": c, \"label\": l})\n",
        "\n",
        "bins = np.linspace(0.44,0.6,50)\n",
        "distplot(correct_scores, bins, 'gray', 'correct')\n",
        "distplot(it2t_ex_scores, bins, 'g', 'it2t failure')\n",
        "distplot(t2t_ex_scores, bins, 'orange', 't2t failure')\n",
        "plt.xlabel(\"embedding coherence score\")\n",
        "plt.legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd0NEercRHpi"
      },
      "outputs": [],
      "source": [
        "distplot = lambda x, b, c, l: sns.distplot(x, kde=True, bins=b,\n",
        "                            kde_kws={\"color\": c, \"lw\": 3, \"alpha\": 0.2},\n",
        "                            hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\n",
        "                            \"alpha\": 0.2, \"color\": c, \"label\": l})\n",
        "\n",
        "bins = np.linspace(0.44,0.6,50)\n",
        "distplot(common_scores, bins, 'gray', 'common')\n",
        "distplot(it2t_ex_scores, bins, 'g', 'it2t specific')\n",
        "distplot(t2t_ex_scores, bins, 'orange', 't2t specific')\n",
        "plt.xlabel(\"embedding coherence score\")\n",
        "plt.legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLaYgU_ySZa7"
      },
      "outputs": [],
      "source": [
        "top_k = 20\n",
        "\n",
        "val_icscores = exs_icwords(all_ex, top_k)\n",
        "it2t_ex_icscores = exs_icwords(it2t_ex, top_k)\n",
        "t2t_ex_icscores = exs_icwords(t2t_ex, top_k)\n",
        "it2t_only_icscores = exs_icwords(it2t_only_ex, top_k)\n",
        "t2t_only_icscores = exs_icwords(t2t_only_ex, top_k)\n",
        "common_icscores = exs_icwords(common_ex, top_k)\n",
        "correct_icscores = exs_icwords(correct_ex, top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmDv26j3SnHV"
      },
      "outputs": [],
      "source": [
        "distplot = lambda x, b, c, l: sns.distplot(x, kde=True, bins=b,\n",
        "                            kde_kws={\"color\": c, \"lw\": 3, \"alpha\": 0.2},\n",
        "                            hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\n",
        "                            \"alpha\": 0.2, \"color\": c, \"label\": l})\n",
        "\n",
        "bins = np.linspace(0,20,50)\n",
        "distplot(correct_icscores, bins, 'gray', 'correct')\n",
        "distplot(it2t_ex_icscores, bins, 'g', 'it2t failure')\n",
        "distplot(t2t_ex_icscores, bins, 'orange', 't2t failure')\n",
        "plt.xlabel(\"% of incoherence tokens\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlim([-3,20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm6-3kwHSprW"
      },
      "outputs": [],
      "source": [
        "distplot = lambda x, b, c, l: sns.distplot(x, kde=True, bins=b,\n",
        "                            kde_kws={\"color\": c, \"lw\": 3, \"alpha\": 0.2},\n",
        "                            hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\n",
        "                            \"alpha\": 0.2, \"color\": c, \"label\": l})\n",
        "\n",
        "bins = np.linspace(0,20,50)\n",
        "distplot(common_icscores, bins, 'gray', 'common')\n",
        "distplot(it2t_only_icscores, bins, 'g', 'it2t specific')\n",
        "distplot(t2t_only_icscores, bins, 'orange', 't2t specific')\n",
        "plt.xlabel(\"% of incoherence tokens\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlim([-3,20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymKG3hCITius"
      },
      "outputs": [],
      "source": [
        "def colored(r, g, b, text):\n",
        "    return \" \\033[38;2;{};{};{}m{}\\033[38;2;255;255;255m \".format(r, g, b, text)\n",
        "\n",
        "def print_diff(ex, score):\n",
        "  rids = []\n",
        "  sentence_ids = input_gen._vocabulary.encode(ex)\n",
        "  for wid in sentence_ids:\n",
        "    rids.append(freq_ids.index(wid))\n",
        "    scores = np.array(score)[rids]\n",
        "  print(' '.join([colored(int(255*scores[i]), int(255*scores[i]), int(255*scores[i]),\n",
        "                          input_gen._vocabulary.decode([ids])) \n",
        "                          for i, ids in enumerate(sentence_ids)]))\n",
        "  return scores\n",
        "\n",
        "def obtain_diff(ex, score):\n",
        "  rids = []\n",
        "  sentence_ids = input_gen._vocabulary.encode(ex)\n",
        "  for wid in sentence_ids:\n",
        "    rids.append(freq_ids.index(wid))\n",
        "    scores = np.array(score)[rids]\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPGOragqT5LT"
      },
      "outputs": [],
      "source": [
        "top_k = 20\n",
        "\n",
        "for ex in it2t_only_ex[:10]:\n",
        "  print_diff(ex, it2t_vs_t2t_k[top_k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi3RIJAiTHgt"
      },
      "outputs": [],
      "source": [
        "top_k = 20\n",
        "\n",
        "for ex in t2t_only_ex[:10]:\n",
        "  print_diff(ex, it2t_vs_t2t_k[top_k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0hOuP6vPvGH"
      },
      "outputs": [],
      "source": [
        "query = 'disappointing'\n",
        "check_word(query, freq_ids, k_nn(sim_matrix_it2t, 20))\n",
        "check_word(query, freq_ids, k_nn(sim_matrix_t2t, 20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HAvRGBmbudO"
      },
      "outputs": [],
      "source": [
        "query = 'coincidence'\n",
        "check_word(query, freq_ids, knn_it2t_10)\n",
        "check_word(query, freq_ids, knn_t2t_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0kJq32abxDp"
      },
      "outputs": [],
      "source": [
        "query = 'facile'\n",
        "check_word(query, freq_ids, knn_it2t_10)\n",
        "check_word(query, freq_ids, knn_t2t_10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63eMlCcpQq9p"
      },
      "source": [
        "## Perturbation-based analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J15I-cxDQhVA"
      },
      "outputs": [],
      "source": [
        "def get_sensitivity(task, input_batch, max_seq_length, eps=0.5):\n",
        "    \n",
        "  p = task.encoder.params\n",
        "  # [batch, time]\n",
        "  input_ids = input_batch.ids\n",
        "  # [batch, time]\n",
        "  paddings = input_batch.paddings\n",
        "\n",
        "  batch = py_utils.GetShape(input_ids)[0]\n",
        "  time = py_utils.GetShape(input_ids)[1]\n",
        "\n",
        "  # Embedding layer.\n",
        "  # [batch, time, dim]\n",
        "  if not p.shared_emb:\n",
        "    input_embs = task.encoder.token_emb.EmbLookup(task.encoder.theta.token_emb, input_ids)\n",
        "  else:\n",
        "    input_embs = task.encoder.softmax.EmbLookup(task.encoder.theta.softmax, input_ids)\n",
        "\n",
        "  perturbed_embs = []\n",
        "  for i in range(max_seq_length + 1):\n",
        "    if i == max_seq_length:\n",
        "      perturbed_embs.append(input_embs)\n",
        "    else:\n",
        "      mask = np.ones((time, p.model_dim))\n",
        "      mask[i, :] += eps\n",
        "      tf.expand_dims(mask, 0)\n",
        "      perturbed_embs.append(mask * input_embs)\n",
        "\n",
        "  perturbed_embs = tf.stack(perturbed_embs)\n",
        "  perturbed_embs = tf.reshape(perturbed_embs,\n",
        "                              [(max_seq_length+1) * batch, time, p.model_dim])\n",
        "\n",
        "  # [1, time, dim]\n",
        "  position_embs = tf.expand_dims(\n",
        "      task.encoder.position_emb.FProp(task.encoder.theta.position_emb, time), 0)\n",
        "\n",
        "  # [batch, time, dim]\n",
        "  perturbed_embs += position_embs\n",
        "\n",
        "  if p.input_dropout_tpl.fprop_dtype:\n",
        "    perturbed_embs = tf.cast(perturbed_embs, p.input_dropout_tpl.fprop_dtype)\n",
        "    paddings = tf.cast(paddings, p.input_dropout_tpl.fprop_dtype)\n",
        "\n",
        "  # [batch, time, dim]\n",
        "  transformer_input = perturbed_embs\n",
        "  # Explicitly set the input shape of Transformer layers, to avoid\n",
        "  # unknown shape error occurred to tf.einsum on nonTPU devices.\n",
        "\n",
        "  transformer_input = tf.reshape(transformer_input,\n",
        "                                  [(max_seq_length+1) * batch, time, p.model_dim])\n",
        "\n",
        "  # Reshape to match with input shapes of other embeddings, e.g. image.\n",
        "  transformer_input = tf.transpose(transformer_input, [1, 0, 2])\n",
        "  paddings = tf.tile(paddings, [max_seq_length+1, 1]) \n",
        "  # paddings = tf.transpose(paddings)\n",
        "\n",
        "  encoder_embeddings = py_utils.NestedMap(input_embs=transformer_input, paddings=paddings)\n",
        "\n",
        "  encoder_outputs = task.encoder.FPropTransformerLayers(task.theta.encoder, \n",
        "                                                        encoder_embeddings)\n",
        "\n",
        "  # decoder\n",
        "  targets = py_utils.NestedMap(ids=tf.tile(sources.ids,  [max_seq_length+1,1]), \n",
        "                               paddings=tf.tile(sources.paddings, [max_seq_length+1,1]))\n",
        "  decoder_outputs = task.decoder.ComputePredictions(task.theta.decoder,\n",
        "                                                    encoder_outputs, targets)\n",
        "\n",
        "  classifier_input = task._extract_classifier_input(\n",
        "      tf.tile(sources.paddings, [max_seq_length+1,1]), decoder_outputs)\n",
        "\n",
        "  predictions = task._apply_classifier(task.theta, classifier_input)\n",
        "\n",
        "  return predictions\n",
        "\n",
        "feed_ids =  tf.placeholder(tf.int32, shape=[1,512])\n",
        "feed_paddings = tf.placeholder(tf.float32, shape=[1,512])\n",
        "\n",
        "sources = py_utils.NestedMap(ids=feed_ids, paddings=feed_paddings)\n",
        "sensitivity_it2t = get_sensitivity(task_it2t, sources, max_seq_length)\n",
        "sensitivity_t2t = get_sensitivity(task_t2t, sources, max_seq_length)\n",
        "\n",
        "sen_it2t = abs(sensitivity_it2t.probs[:,0] - sensitivity_it2t.probs[-1][0])\n",
        "sen_t2t = abs(sensitivity_t2t.probs[:,0] - sensitivity_t2t.probs[-1][0])\n",
        "\n",
        "# Notice that we are calling this with task.theta which ensures that we are\n",
        "# using the same variables which we have just loaded.\n",
        "fetches = py_utils.NestedMap(\n",
        "          {\"sources\": sources,\n",
        "           \"prob_it2t\": sensitivity_it2t.probs,\n",
        "           \"prob_t2t\": sensitivity_t2t.probs,\n",
        "           \"sensitivity_it2t\":sen_it2t,\n",
        "           \"sensitivity_t2t\":sen_t2t\n",
        "           })\n",
        "\n",
        "print(fetches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWQc_s5Rv07_"
      },
      "outputs": [],
      "source": [
        "ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBTDh2dev5JD"
      },
      "outputs": [],
      "source": [
        "np.tile(ids, [30,1]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0LEoa8efeKf"
      },
      "outputs": [],
      "source": [
        "def process_ex(ex):\n",
        "  inputs = input_gen._vocabulary._encode(ex)\n",
        "  ids = np.pad(inputs, (1, 511-len(inputs)), 'constant', \n",
        "               constant_values=(0, 0)).reshape(1,-1)\n",
        "  paddings = np.pad(np.zeros(len(inputs)+1), (0, 511-len(inputs)), \n",
        "                    'constant', constant_values=(1, 1)).reshape(1,-1)\n",
        "  input_len = len(inputs) + 1\n",
        "\n",
        "  return ids, paddings, input_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqSUc8i-MSsx"
      },
      "outputs": [],
      "source": [
        "def print_sensitivity(ex, score):\n",
        "  rids = []\n",
        "  sentence_ids = input_gen._vocabulary.encode(ex)\n",
        "  for wid in sentence_ids:\n",
        "    rids.append(freq_ids.index(wid))\n",
        "    scores = 1-np.array(score)\n",
        "  print(' '.join([colored(int(255*scores[i]), int(255*scores[i]), int(255*scores[i]),\n",
        "                          input_gen._vocabulary.decode([ids])) \n",
        "                          for i, ids in enumerate(sentence_ids)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX5yExbtCq_6"
      },
      "outputs": [],
      "source": [
        "it2t_ex[0] in all_ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58EMJ-3ZM6Fk"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "test_outputs = []\n",
        "\n",
        "for ex in it2t_only_ex[:10]:\n",
        "  ids, paddings, input_len = process_ex(ex)\n",
        "  max_seq_length = max(max_seq_length, input_len)\n",
        "  test_outputs.append(sess.run(fetches, {feed_ids: ids, feed_paddings: paddings}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GagztmNZM6Fw"
      },
      "outputs": [],
      "source": [
        "def print_sensitivity(ex, score):\n",
        "  rids = []\n",
        "  sentence_ids = input_gen._vocabulary.encode(ex)\n",
        "  for wid in sentence_ids:\n",
        "    rids.append(freq_ids.index(wid))\n",
        "    scores = 1-np.array(score)\n",
        "  print(' '.join([colored(int(255*scores[i]), int(255*scores[i]), int(255*scores[i]),\n",
        "                          input_gen._vocabulary.decode([ids])) \n",
        "                          for i, ids in enumerate(sentence_ids)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLu28XQLQH1A"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MZ9RWoQGBpm"
      },
      "outputs": [],
      "source": [
        "len(it2t_ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTg0JvP0Gx6E"
      },
      "outputs": [],
      "source": [
        "len(t2t_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8XL4GakKTtz"
      },
      "source": [
        "### activation based analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS5qYKG_LN6Z"
      },
      "outputs": [],
      "source": [
        "max_seq_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN-UseLiP_yf"
      },
      "outputs": [],
      "source": [
        "def get_sensitivity(task, input_batch, max_seq_length, eps=0.5):\n",
        "    \n",
        "  p = task.encoder.params\n",
        "  # [batch, time]\n",
        "  input_ids = input_batch.ids\n",
        "  # [batch, time]\n",
        "  paddings = input_batch.paddings\n",
        "\n",
        "  batch = py_utils.GetShape(input_ids)[0]\n",
        "  time = py_utils.GetShape(input_ids)[1]\n",
        "\n",
        "  # Embedding layer.\n",
        "  # [batch, time, dim]\n",
        "  if not p.shared_emb:\n",
        "    input_embs = task.encoder.token_emb.EmbLookup(task.encoder.theta.token_emb, input_ids)\n",
        "  else:\n",
        "    input_embs = task.encoder.softmax.EmbLookup(task.encoder.theta.softmax, input_ids)\n",
        "\n",
        "  perturbed_embs = []\n",
        "  for i in range(max_seq_length + 1):\n",
        "    if i == max_seq_length:\n",
        "      perturbed_embs.append(input_embs)\n",
        "    else:\n",
        "      mask = np.ones((time, p.model_dim))\n",
        "      mask[i, :] += eps\n",
        "      tf.expand_dims(mask, 0)\n",
        "      perturbed_embs.append(mask * input_embs)\n",
        "\n",
        "  perturbed_embs = tf.stack(perturbed_embs)\n",
        "  perturbed_embs = tf.reshape(perturbed_embs,\n",
        "                              [(max_seq_length+1) * batch, time, p.model_dim])\n",
        "\n",
        "  # [1, time, dim]\n",
        "  position_embs = tf.expand_dims(\n",
        "      task.encoder.position_emb.FProp(task.encoder.theta.position_emb, time), 0)\n",
        "\n",
        "  # [batch, time, dim]\n",
        "  perturbed_embs += position_embs\n",
        "\n",
        "  if p.input_dropout_tpl.fprop_dtype:\n",
        "    perturbed_embs = tf.cast(perturbed_embs, p.input_dropout_tpl.fprop_dtype)\n",
        "    paddings = tf.cast(paddings, p.input_dropout_tpl.fprop_dtype)\n",
        "\n",
        "  # [batch, time, dim]\n",
        "  transformer_input = perturbed_embs\n",
        "  # Explicitly set the input shape of Transformer layers, to avoid\n",
        "  # unknown shape error occurred to tf.einsum on nonTPU devices.\n",
        "\n",
        "  transformer_input = tf.reshape(transformer_input,\n",
        "                                  [(max_seq_length+1) * batch, time, p.model_dim])\n",
        "\n",
        "  # Reshape to match with input shapes of other embeddings, e.g. image.\n",
        "  transformer_input = tf.transpose(transformer_input, [1, 0, 2])\n",
        "  paddings = tf.tile(paddings, [max_seq_length+1, 1]) \n",
        "  paddings = tf.transpose(paddings)\n",
        "\n",
        "  encoder_embeddings = py_utils.NestedMap(input_embs=transformer_input, paddings=paddings)\n",
        "\n",
        "  encoder_outputs = task.encoder.FPropTransformerLayers(task.theta.encoder, \n",
        "                                                        encoder_embeddings)\n",
        "\n",
        "  # decoder\n",
        "  targets = py_utils.NestedMap(ids=tf.tile(sources.ids,  [max_seq_length+1,1]), \n",
        "                               paddings=tf.tile(sources.paddings, [max_seq_length+1,1]))\n",
        "  decoder_outputs = task.decoder.ComputePredictions(task.theta.decoder,\n",
        "                                                    encoder_outputs, targets)\n",
        "\n",
        "  classifier_input = task._extract_classifier_input(\n",
        "      tf.tile(sources.paddings, [max_seq_length+1,1]), decoder_outputs)\n",
        "\n",
        "  predictions = task._apply_classifier(task.theta, classifier_input)\n",
        "\n",
        "  return predictions\n",
        "\n",
        "feed_ids =  tf.placeholder(tf.int32, shape=[1,512])\n",
        "feed_paddings = tf.placeholder(tf.float32, shape=[1,512])\n",
        "\n",
        "sources = py_utils.NestedMap(ids=feed_ids, paddings=feed_paddings)\n",
        "sensitivity_it2t = get_sensitivity(task_it2t, sources, max_seq_length)\n",
        "sensitivity_t2t = get_sensitivity(task_t2t, sources, max_seq_length)\n",
        "\n",
        "sen_it2t = abs(sensitivity_it2t.probs[:,0] - sensitivity_it2t.probs[-1][0])\n",
        "sen_t2t = abs(sensitivity_t2t.probs[:,0] - sensitivity_t2t.probs[-1][0])\n",
        "\n",
        "# Notice that we are calling this with task.theta which ensures that we are\n",
        "# using the same variables which we have just loaded.\n",
        "fetches = py_utils.NestedMap(\n",
        "          {\"sources\": sources,\n",
        "           \"prob_it2t\": sensitivity_it2t.probs,\n",
        "           \"prob_t2t\": sensitivity_t2t.probs,\n",
        "           \"sensitivity_it2t\":sen_it2t,\n",
        "           \"sensitivity_t2t\":sen_t2t\n",
        "           })\n",
        "\n",
        "print(fetches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK4J7tfJKf6J"
      },
      "outputs": [],
      "source": [
        "def process_ex(ex):\n",
        "  inputs = input_gen._vocabulary._encode(ex)\n",
        "  ids = np.pad(inputs, (1, 511-len(inputs)), 'constant', \n",
        "               constant_values=(0, 0)).reshape(1,-1)\n",
        "  paddings = np.pad(np.zeros(len(inputs)+1), (0, 511-len(inputs)), \n",
        "                    'constant', constant_values=(1, 1)).reshape(1,-1)\n",
        "  input_len = len(inputs) + 1\n",
        "\n",
        "  return ids, paddings, input_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78WDiyu-KwFn"
      },
      "outputs": [],
      "source": [
        "correct_test_outputs = []\n",
        "it2t_only_test_outputs = []\n",
        "t2t_only_test_outputs = []\n",
        "\n",
        "# max_seq_length = 0\n",
        "\n",
        "for ex in it2t_only_ex:\n",
        "  ids, paddings, input_len = process_ex(ex)\n",
        "  # max_seq_length = max(max_seq_length, input_len)\n",
        "  it2t_only_test_outputs.append(sess.run(fetches, {feed_ids: ids, feed_paddings: paddings}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPhEiG44SrA_"
      },
      "outputs": [],
      "source": [
        "for ex in t2t_only_ex:\n",
        "  ids, paddings, input_len = process_ex(ex)\n",
        "  # max_seq_length = max(max_seq_length, input_len)\n",
        "  t2t_only_test_outputs.append(sess.run(fetches, {feed_ids: ids, feed_paddings: paddings}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laciSXspV12P"
      },
      "outputs": [],
      "source": [
        "for ex in correct_ex[:30]:\n",
        "  ids, paddings, input_len = process_ex(ex)\n",
        "  # max_seq_length = max(max_seq_length, input_len)\n",
        "  correct_test_outputs.append(sess.run(fetches, {feed_ids: ids, feed_paddings: paddings}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nHozKTCKwFp"
      },
      "outputs": [],
      "source": [
        "correct_sscore_it2t = [correct_test_outputs[ids].sensitivity_it2t[correct_test_outputs[ids].sources.paddings[0][:85] == 0][1:] for ids in range(30)]\n",
        "correct_sscore_t2t = [correct_test_outputs[ids].sensitivity_t2t[correct_test_outputs[ids].sources.paddings[0][:85] == 0][1:] for ids in range(30)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGXPwSCAmoQp"
      },
      "outputs": [],
      "source": [
        "it2t_sscore_it2t = [it2t_only_test_outputs[ids].sensitivity_it2t[it2t_only_test_outputs[ids].sources.paddings[0][:85] == 0][1:] for ids in range(len(it2t_only_ex))]\n",
        "it2t_sscore_t2t = [it2t_only_test_outputs[ids].sensitivity_t2t[it2t_only_test_outputs[ids].sources.paddings[0][:85] == 0][1:] for ids in range(len(it2t_only_ex))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X__0UTmdm3ZY"
      },
      "outputs": [],
      "source": [
        "t2t_sscore_it2t = [t2t_only_test_outputs[ids].sensitivity_it2t[t2t_only_test_outputs[ids].sources.paddings[0][:85] == 0][1:] for ids in range(len(t2t_only_ex))]\n",
        "t2t_sscore_t2t = [t2t_only_test_outputs[ids].sensitivity_t2t[t2t_only_test_outputs[ids].sources.paddings[0][:85] == 0][1:] for ids in range(len(t2t_only_ex))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7BGg1hdm_Wp"
      },
      "outputs": [],
      "source": [
        "correct_ic = []\n",
        "it2t_only_ic = []\n",
        "t2t_only_ic = []\n",
        "\n",
        "for ex in it2t_only_ex:\n",
        "  it2t_only_ic.append(obtain_diff(ex, it2t_vs_t2t_k[top_k]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O-mZ6uqtEbY"
      },
      "outputs": [],
      "source": [
        "for ex in t2t_only_ex:\n",
        "  t2t_only_ic.append(obtain_diff(ex, it2t_vs_t2t_k[top_k]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBbRcmP2tFUf"
      },
      "outputs": [],
      "source": [
        "for ex in correct_ex[:30]:\n",
        "  correct_ic.append(obtain_diff(ex, it2t_vs_t2t_k[top_k]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yWlcrjatNsc"
      },
      "outputs": [],
      "source": [
        "top_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFXYlFuhtQO-"
      },
      "outputs": [],
      "source": [
        "r2_correct_it2t = [r2(correct_ic[i], correct_sscore_it2t[i]) for i in range(30)]\n",
        "r2_it2t_it2t = [r2(it2t_only_ic[i], it2t_sscore_it2t[i]) for i in range(len(it2t_only_ex))]\n",
        "r2_t2t_it2t = [r2(t2t_only_ic[i], t2t_sscore_it2t[i]) for i in range(len(t2t_only_ex))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcTdAP35tQSN"
      },
      "outputs": [],
      "source": [
        "r2_correct_t2t = [r2(correct_ic[i], correct_sscore_t2t[i]) for i in range(30)]\n",
        "r2_it2t_t2t = [r2(it2t_only_ic[i], it2t_sscore_t2t[i]) for i in range(len(it2t_only_ex))]\n",
        "r2_t2t_t2t = [r2(t2t_only_ic[i], t2t_sscore_t2t[i]) for i in range(len(t2t_only_ex))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-oRzcvGtQU0"
      },
      "outputs": [],
      "source": [
        "np.mean(r2_correct_it2t), np.mean(r2_correct_t2t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LWTiy65tQXW"
      },
      "outputs": [],
      "source": [
        "np.mean(r2_it2t_it2t), np.mean(r2_it2t_t2t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJUEVXuEtQbf"
      },
      "outputs": [],
      "source": [
        "np.mean(r2_t2t_it2t), np.mean(r2_t2t_t2t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_uqaDfcwN95"
      },
      "outputs": [],
      "source": [
        "df = {\"correct\": r2_correct_it2t, \"it2t correct\\nt2t wrong\": r2_t2t_it2t, \"it2t wrong\\nt2t correct\": r2_it2t_it2t,}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhhUEjksxZd3"
      },
      "outputs": [],
      "source": [
        "df = dict2pandas(df, \"examples\", \"Pearson r^2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQScSKmFwwTc"
      },
      "outputs": [],
      "source": [
        "ax = sns.violinplot(data=df, x='examples', y='Pearson r^2')\n",
        "for violin, alpha in zip(ax.collections[::2], [0.4,0.4,0.4]):\n",
        "    violin.set_alpha(alpha)\n",
        "Means = df.groupby('examples')['Pearson r^2'].median()\n",
        "plt.scatter(x=range(len(Means)),y=Means,c=\"white\", zorder=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajoxY3rFzzfs"
      },
      "outputs": [],
      "source": [
        "Means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VQ2DI50vuG6"
      },
      "outputs": [],
      "source": [
        "def dict2pandas(d, keyname, valname):\n",
        "    dframes = []\n",
        "    for k,v in d.items():\n",
        "        dframes += [pd.DataFrame({keyname : [k] * len(v), valname : v})]\n",
        "    return pd.concat(dframes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_Gan-20KwFp"
      },
      "outputs": [],
      "source": [
        "ids = 6\n",
        "\n",
        "plt.plot(sscore_it2t[ids])\n",
        "plt.plot(sscore_t2t[ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_4IE5VYOXAF"
      },
      "outputs": [],
      "source": [
        "top_k = 20\n",
        "\n",
        "for i, ex in enumerate(it2t_only_ex[:1]):\n",
        "  print_diff(ex, it2t_vs_t2t_k[top_k])\n",
        "  print_sensitivity(ex, 5* it2t_sscore_it2t[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYGfFXxLOxqK"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "def r2(x, y):\n",
        "    return stats.pearsonr(x, y)[0] ** 2\n",
        "\n",
        "r2(ex_ic_scores, sscore_it2t[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzOojJQePF5N"
      },
      "outputs": [],
      "source": [
        "r2(ex_ic_scores, sscore_t2t[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xmQJyqDPwhm"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "Mixed Embedding/Attention inspection (local, SST2).ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1kzRjeYFGgPHG4ax28FVUjsOkbuNAsvzl",
          "timestamp": 1630034343522
        },
        {
          "file_id": "1b_Q5bA5GX2BAG_xjUsIaqyGsG3Evp-li",
          "timestamp": 1629752149416
        },
        {
          "file_id": "18YlgoYEycxsLfO-Q7i2gAkmWYXCsKHLa",
          "timestamp": 1626109854299
        },
        {
          "file_id": "1LgQaLkIgTAMix0PrNiBc8DnHk66U324g",
          "timestamp": 1624467180657
        },
        {
          "file_id": "1DWQpm9DyYFhZTUL28PSCtUIugu7pZTTD",
          "timestamp": 1623879707760
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
